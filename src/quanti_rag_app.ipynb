{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import tempfile\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from ctransformers import AutoModelForCausalLM,AutoConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\"llama-2-7b-chat.ggmlv3.q8_0.bin\")\n",
    "# Explicitly set the max_seq_len\n",
    "config.max_seq_len = 4096\n",
    "config.max_answer_len= 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "prompts = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the locally downloaded model\n",
    "def load_llm():\n",
    "    llm = CTransformers(\n",
    "        model=\"llama-2-7b-chat.ggmlv3.q8_0.bin\",\n",
    "        model_type=\"llama\",\n",
    "        max_new_tokens=2048,\n",
    "        temperature=0.2,\n",
    "        max_length=2048,\n",
    "        top_p = .95,\n",
    "        top_k = 40\n",
    "    )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, page_content, metadata=None):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata if metadata is not None else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_file = 'ConceptsofBiology-WEB.pdf'\n",
    "\n",
    "if uploaded_file:\n",
    "    # Read the PDF file\n",
    "    pdfReader = PdfReader(uploaded_file)\n",
    "    documents = []\n",
    "    for page in pdfReader.pages[:68]:  # Only take first 2 chapter\n",
    "        page_text = page.extract_text()\n",
    "        documents.append(Document(page_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into Chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "text_chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embeddings and vectorstore\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': 'cpu'})\n",
    "db = FAISS.from_documents(text_chunks, embeddings)\n",
    "db.save_local(DB_FAISS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and set up the retrieval chain\n",
    "llm = load_llm()\n",
    "chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=db.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This file appears to be related to research on large datasets and the use of Creative Commons licenses. The mention of \"data research\" and \"in silico research\" suggests that the file may be related to the analysis of large data sets using computational methods, rather than traditional experimental approaches.\n"
     ]
    }
   ],
   "source": [
    "# Define a function to handle  chat\n",
    "def conversational_chat(query):\n",
    "    max_length = 2048\n",
    "    chat_history = [] \n",
    "    answer = \"\"\n",
    "    chunks = [query[i:i+max_length] for i in range(0, len(query), max_length)]\n",
    "    for chunk in chunks:\n",
    "        result = chain({\"question\": chunk, \"chat_history\": chat_history})\n",
    "        answer += result[\"answer\"]\n",
    "    return answer\n",
    "\n",
    "user_input = \"what is this file all about?\"\n",
    "if user_input:\n",
    "    output = conversational_chat(user_input)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
